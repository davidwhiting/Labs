
\label{prob:newton-basic}
Write a function that accepts a function $f$, an initial guess $x_0$, the derivative $f'$, a stopping tolerance defaulting to $10^{-5}$, and a maximum number of iterations defaulting to $15$.
Use Newton's method as described in (\ref{eq:newton-1d-def}) to compute a zero $\bar{x}$ of $f$.
Terminate the algorithm when $|x_k - x_{k-1}|$ is less than the stopping tolerance or after iterating the maximum number of allowed times.
Return the last computed approximation to $\bar{x}$, a boolean value indicating whether or not the algorithm converged, and the number of iterations completed.

Test your function against functions like $f(x) = e^x - 2$ (see Figure \ref{fig:newton}) or $f(x) = x^4 - 3$.
Check that the computed zero $\bar{x}$ satisfies $f(\bar{x}) \approx 0$.
Also consider comparing your function to \li{scipy.optimize.newton()}, which accepts similar arguments.
 % Application to interest rates.
\label{prob:newton-interest}
Suppose that an amount of $P_1$ dollars is put into an account at the beginning of years $1, 2,..., N_1$ and that the account accumulates interest at a fractional rate $r$ (so $r = .05$ corresponds to $5\%$ interest).
In addition, at the beginning of years $N_1 + 1, N_1 + 2, ..., N_1 + N_2$, an amount of $P_2$ dollars is withdrawn from the account and that the account balance is exactly zero after the withdrawal at year $N_1 + N_2$.
Then the variables satisfy
\[
P_1[(1+r)^{N_1} - 1] = P_2[1-(1+r)^{-N_2}].
\]

Write a function that, given $N_1$, $N_2$, $P_1$, and $P_2$, uses Newton's method to determine $r$.
For the initial guess, use $r_0 = 0.1$.
\\(Hint: Construct $f(r)$ such that when $f(r)=0$, the equation is satisfied.
Also compute $f'(r)$.)

To test your function, if $N_1 =30, N_2 =20, P_1 =2000$, and $P_2 =8000$, then $r\approx 0.03878$.
(From Atkinson, page 118). % TODO: better citation.
 % Implement backtracking.
Modify your function from Problem \ref{prob:newton-basic} so that it accepts a parameter $\alpha$ that defaults to $1$.
Incorporate (\ref{eq:newton-backtracking-1d}) to allow for backtracking.

To test your modified function, consider $f(x)=x^{1/3}$.
The command \li{x**(1/3.)} fails when \li{x} is negative, so the function can be defined with NumPy as follows.
\begin{lstlisting}
import numpy as np
f = lambda x: np.sign(x) * np.power(np.<<abs>>(x), 1./3)
\end{lstlisting}
With $x_0=.01$ and $\alpha=1$, the iteration should \textbf{not} converge.
However, setting $\alpha=.4$, the iteration should converge to a zero that is close to $0$.
\label{prob:newton-1d-backtracking}
 % Searching for a good backtracking constant.
\label{prob:newton-backtracking-search}
Write a function that accepts the same arguments as your function from Problem \ref{prob:newton-1d-backtracking} except for $\alpha$.
Use Newton's method to find a zero of $f$ using various values of $\alpha$ in the interval $(0,1]$.
Plot the values of $\alpha$ against the number of iterations performed by Newton's method.
Return a value for $\alpha$ that results in the lowest number of iterations.

A good test case for this problem is the function $f(x) = x^{1/3}$ discussed in Problem \ref{prob:newton-1d-backtracking}.
In this case, your plot should show that the optimal value for $\alpha$ is actually closer to $.3$ than to $.4$.
% resemble the following figure.
% \begin{figure}[H]
% \includegraphics[width=.7\textwidth]{figures/alpha_iters.pdf}
% \end{figure}
 % Newton's method in n dimensions.
Modify your function from Problems \ref{prob:newton-basic} and \ref{prob:newton-1d-backtracking} so that it can compute a zero of a function $f:\mathbb{R}^n\rightarrow\mathbb{R}^n$ for any $n\in\mathbb{N}$.
Take the following tips into consideration.
\begin{itemize}
\item If $n > 1$, $f$ should be a function that accepts a 1-D NumPy array with $n$ entries and returns another NumPy array with $n$ entries.
Similarly, $Df$ should be a function that accepts a 1-D array with $n$ entries and returns a $n\times n$ array.
In other words, $f$ and $Df$ are callable functions, but $f(\x)$ is a vector and $Df(\x)$ is a matrix.

\item \li{np.isscalar()} may be useful for determining whether or not $n > 1$.

\item Instead of computing $Df(\x_k)^{-1}$ directly at each step, solve the system $Df(\x_k)\y_k = f(\x_k)$ and set $\x_{k+1} = \x_k - \alpha\y_k$.
Always avoid taking matrix inverses when possible.

\item The stopping criterion now requires using a norm function instead of \li{abs()}.
\end{itemize}

After your modifications, carefully verify that your function still works in the case that $n=1$, and that your functions from Problems \ref{prob:newton-interest} and \ref{prob:newton-backtracking-search} also still work correctly.
In addition, your function from Problem \ref{prob:newton-backtracking-search} should also work for any $n \in N$.
\label{prob:newton-nd-implementation}

Bioremediation involves the use of bacteria to consume toxic wastes.
At a steady state, the bacterial density $x$ and the nutrient concentration $y$ satisfy the system of nonlinear equations
\begin{align*}
\gamma xy - x(1 + y) &= 0 \\
-xy + (\delta - y)(1 + y) &= 0,
\end{align*}
where $\gamma$ and $\delta$ are parameters that depend on various physical features of the system.

For this problem, assume the typical values $\gamma = 5$ and $\delta = 1$, for which the system has solutions at $(x, y) = (0, 1), (0, -1)$, and $(3.75, .25)$.
Write a function that finds an initial point $\x_0 = (x_0,y_0)$ such that Newton's method converges to either $(0, 1)$ or $(0, -1)$ with $\alpha = 1$, and to $(3.75, .25)$ with $\alpha = 0.55$.
As soon as a valid $\x_0$ is found, return it (stop searching).
\\(Hint: search within the rectangle $[-\frac{1}{4},0]\times[0,\frac{1}{4}]$.)

(Adapted from problem 5.19 of M. T. Heath, Scientific Computing, an Introductory Survey, 2nd edition, McGraw Hill, 2002 and the Notes of Homer Walker). % TODO: a better citation.
 % Plot the basins of attraction.
Write a function that accepts a function $f:\mathbb{C}\rightarrow\mathbb{C}$, its derivative $f':\mathbb{C}\rightarrow\mathbb{C}$, an array \li{zeros} of the zeros of $f$, bounds $[r_{\text{min}},r_{\text{max}},i_{\text{min}},i_{\text{max}}]$ for the domain of the plot, an integer \li{res} that determines the resolution of the plot, and number of iterations \li{iters} to run the iteration.
Compute and plot the basins of attraction of $f$ in the complex plane over the specified domain in the following steps.
\begin{enumerate}
\item Construct a \li{res}$\times$\li{res} grid $X_0$ over the domain $\{a+bi \mid a \in [r_{\text{min}},r_{\text{max}}], b \in [i_{\text{min}},i_{\text{max}}]\}$.

\item Run Newton's method (without backtracking) on $X_0$ \li{iters} times, obtaining the \li{res}$\times$\li{res} array $X_{k}$.
To avoid the additional computation of checking for convergence at each step, do not use your function from Problem \ref{prob:newton-nd-implementation}.

\item $X_k$ cannot be directly visualized directly because its values are complex.
Solve this issue by creating another \li{res}$\times$\li{res} array $Y$.
To compute the $(i,j)$th entry $Y_{i,j}$, determine which zero of $f$ is closest to the $(i,j)$th entry of $X_k$.
Set $Y_{i,j}$ to the index of this zero in the array \li{zeros}.
If there are $R$ distinct zeros, each $Y_{i,j}$ should be one of $0,1,\ldots,R-1$.
\\(Hint: \li{np.argmin()} may be useful.)

\item Use \li{plt.pcolormesh()} to visualize the basins.
Recall that this function accepts three array arguments: the $x$-coordinates (in this case, the real components of the initial grid), the $y$-coordinates (the imaginary components of the grid), and an array indicating color values ($Y$).
Set \li{cmap="brg"} to get the same color scheme as in Figure \ref{fig:newton-basins}.
\end{enumerate}

Test your function using $f(x) = x^3-1$ and $f(x)=x^3-x$.
The resulting plots should resemble Figures \ref{fig:fractal_hw} and \ref{fig:fractal_ex}, respectively.

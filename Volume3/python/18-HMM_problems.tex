
\begin{comment}
The following was listed as a problem, but gave the student nothing to do, so we removed the given code.

To start off your implementation of the HMM, define a class using the following code.
You will be adding class methods throughout the remainder of the lab.
\begin{lstlisting}
class hmm(object):
    """
    Finite state space hidden markov model.
    """
    def __init__(self):
        """
        Initialize model parameters.
        """
        self.A = None
        self.B = None
        self.pi = None
\end{lstlisting}
\end{comment}
To start off your implementation of the HMM, define a class object which you should call ``hmm".
Then add the initialization method, in which you should set the \emph{self} aspects A, B, and pi to be None objects.
You will be adding methods throughout the remainder of the lab.

Implement the forward pass by adding the following method to your class:
\begin{lstlisting}
def _forward(self, obs):
    """
    Compute the scaled forward probability matrix and scaling factors.

    Parameters
    ----------
    obs : ndarray of shape (T,)
        The observation sequence

    Returns
    -------
    alpha : ndarray of shape (T,N)
        The scaled forward probability matrix
    c : ndarray of shape (T,)
        The scaling factors c = [c_1,c_2,...,c_T]
    """
    pass
\end{lstlisting}
To verify that your code works, you should get the following output using the toy HMM:
\begin{lstlisting}
>>> h = hmm()
>>> h.A = A
>>> h.B = B
>>> h.pi = pi
>>> alpha, c = h._forward(obs)
>>> print -(np.log(c)).sum() # the log prob of observation
-4.6429135909
\end{lstlisting}

Implement the backward pass by adding the following method to your class:
\begin{lstlisting}
def _backward(self, obs, c):
    """
    Compute the scaled backward probability matrix.

     Parameters
    ----------
    obs : ndarray of shape (T,)
        The observation sequence
    c : ndarray of shape (T,)
        The scaling factors from the forward pass

    Returns
    -------
    beta : ndarray of shape (T,N)
        The scaled backward probability matrix
    """
    pass
\end{lstlisting}
Using the same toy example as before, your code should produce the following output:
\begin{lstlisting}
>>> beta = h._backward(obs, c)
>>> print beta
[[ 3.1361635   2.89939354]
 [ 2.86699344  4.39229044]
 [ 3.898812    2.66760821]
 [ 3.56816483  3.56816483]]
\end{lstlisting}

Add the following method to your class to compute the $\delta$ and $\gamma$ probabilities.
\begin{lstlisting}
def _delta(self, obs, alpha, beta):
    """
    Compute the delta probabilities.

    Parameters
    ----------
    obs : ndarray of shape (T,)
        The observation sequence
    alpha : ndarray of shape (T,N)
        The scaled forward probability matrix from the forward pass
    beta : ndarray of shape (T,N)
        The scaled backward probability matrix from the backward pass

    Returns
    -------
    delta : ndarray of shape (T-1,N,N)
        The delta probability array
    gamma : ndarray of shape (T,N)
        The gamma probability array
    """
    pass
\end{lstlisting}
While writing a triply-nested loop may be the simplest way to convert the formula into code,
it is possible to use array broadcasting to eliminate two of the loops, which will speed up your code.

Check your code by making sure it produces the following output, using the same toy example as before.
\begin{lstlisting}
>>> delta, gamma = h._delta(obs, alpha, beta)
>>> print delta
[[[ 0.14166321  0.0465066 ]
  [ 0.37776855  0.43406164]]

 [[ 0.17015868  0.34927307]
  [ 0.05871895  0.4218493 ]]

 [[ 0.21080834  0.01806929]
  [ 0.59317106  0.17795132]]]
>>> print gamma
[[ 0.18816981  0.81183019]
 [ 0.51943175  0.48056825]
 [ 0.22887763  0.77112237]
 [ 0.8039794   0.1960206 ]]
\end{lstlisting}

Implement the parameter update step by adding the following method to your class:
\begin{lstlisting}
def _estimate(self, obs, delta, gamma):
    """
    Estimate better parameter values.

    Parameters
    ----------
    obs : ndarray of shape (T,)
        The observation sequence
    delta : ndarray of shape (T-1,N,N)
        The delta probability array
    gamma : ndarray of shape (T,N)
        The gamma probability array
    """
    # update self.A, self.B, self.pi in place
    pass
\end{lstlisting}
Verify that your code produces the following output on the toy HMM from before:
\begin{lstlisting}
h._estimate(obs, delta)
>>> print h.A
[[ 0.55807991  0.49898142]
 [ 0.44192009  0.50101858]]
>>> print h.B
[[ 0.23961928  0.70056364]
 [ 0.29844534  0.21268397]
 [ 0.46193538  0.08675238]]
>>> print h.pi
[ 0.18816981  0.81183019]
\end{lstlisting}

Implement the learning algorithm by adding the following method to your class:
\begin{lstlisting}
def fit(self, obs, A, B, pi, max_iter=100, tol=1e-3):
    """
    Fit the model parameters to a given observation sequence.

    Parameters
    ----------
    obs : ndarray of shape (T,)
        Observation sequence on which to train the model.
    A : stochastic ndarray of shape (N,N)
        Initialization of state transition matrix
    B : stochastic ndarray of shape (M,N)
        Initialization of state observation matrix
    pi : stochastic ndarray of shape (N,)
        Initialization of initial state distribution
    max_iter : integer
        The maximum number of iterations to take
    tol : float
        The convergence threshold for change in log-probability
    """
    # initialize self.A, self.B, self.pi
    # run the iteration
    pass
\end{lstlisting}

You are now ready to train a HMM using the Declaration of Independence data.
Use $N=2$ states and $M=$\li{len(set(obs))}$=27$ observation values (26 lower case characters and 1 whitespace character),
and run for 200 iterations with the default value for \li{tol}.
Generally speaking, if you converge to a log probability greater than $-21550$, then you have reached
an acceptable set of parameters for this dataset.

Once the learning algorithm converges, analyze the state observation matrix $B$.
Note which rows correspond to the largest and smallest probability values in each column of $B$,
and check the corresponding characters.
The code below displays typical results for a well-converged HMM.  Note that the \li{u} before the \li{"} indicates that the string should be unicode, which will be required for languages other than English.
\begin{lstlisting}
>>> for i in xrange(len(h.B)):
>>>     print  u"{0}, {1:0.4f}, {2:0.4f}".format(symbols[i], h.B[i,0], h.B[i,1])
 , 0.0051, 0.3324
a, 0.0000, 0.1247
c, 0.0460, 0.0000
b, 0.0237, 0.0000
e, 0.0000, 0.2245
d, 0.0630, 0.0000
g, 0.0325, 0.0000
f, 0.0450, 0.0000
i, 0.0000, 0.1174
h, 0.0806, 0.0070
k, 0.0031, 0.0005
j, 0.0040, 0.0000
m, 0.0360, 0.0000
l, 0.0569, 0.0001
o, 0.0009, 0.1331
n, 0.1207, 0.0000
q, 0.0015, 0.0000
p, 0.0345, 0.0000
s, 0.1195, 0.0000
r, 0.1062, 0.0000
u, 0.0000, 0.0546
t, 0.1600, 0.0000
w, 0.0242, 0.0000
v, 0.0185, 0.0000
y, 0.0147, 0.0058
x, 0.0022, 0.0000
z, 0.0010, 0.0000
\end{lstlisting}
What do you notice about the second column of $B$? It seems that the HMM has detected a vowel state and a consonant state, without any prior input from an English speaker.
Interestingly, the whitespace character is grouped together with the vowels. A HMM can also detect the vowel/consonant distinction in other languages. 

Repeat the previous calculation with 3 hidden states and again with 4 hidden states.  Interpret/explain your results.

Repeat the calculations for 2, and 3 hidden states for \texttt{WarAndPeace.txt}.
Interpret/explain your results.  Which Cyrillic characters appear to be vowels? 

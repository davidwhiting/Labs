
Write a function \li{initialize()} that initializes a Client object, creates a Direct View with all available engines, and imports \li{scipy.sparse as spar} on all engines.

Write a function \li{variables(dx)} that accepts a dictionary of variables. 
Create a \li{Client} object and a \li{Direct View} and distribute the variables.
Then, pull the variables in both a blocking and non-blocking format.
Return the resultant objects of both formats.

Using one of the \texttt{apply} methods, write a function \li{apply_dist(n=1000000)} that prints the mean, max, and min of $n$ draws on each engine from the standard normal distribution.
For example if you have four engines running, your output should look like:
\begin{lstlisting}
<< means = [0.0031776784, -0.0058112042, 0.0012574772, -0.0059655951] >>
<< maxs = [4.0388107, 4.3664958, 4.2060184, 4.3391623] >>
<< mins = [-4.1508589, -4.3848019, -4.1313324, -4.2826519] >>
\end{lstlisting}

Using the function you wrote in the previous problem, compare the time it takes to run the function with parallel computing to the time it takes to run the function serially.
That is, time how long it takes to run the function on all of your machine's engines simultaneously and how long it takes to run the function in a \texttt{for} loop $n$ times, where $n$ is the number of engines on your machine.
Print the results for 1,000,000; 5,000,000; 10,000,000; and 15,000,000 samples.
You should notice an increase in efficiency as the problem size increases.

In a previous lab, latitude and longitude points of recycle bins and addresses in New York City were analyzed to find the address furthest from a recycle bin.
To do so, a KDTree was implemented with the following:

\begin{lstlisting}
>>> from scipy.spatial import KDTree
# Columns should be latitude then longitude
>>> lat_long_array = np.array([ [1,2], [2,3], [3,4] ])
>>> tree = KDTree(lat_long_array)
>>> sample_point = np.array([2,5])
# Queries can be made with
>>> q = tree.query(sample_point)
>>> q
<< (1.4142135623730951, 2) >>
\end{lstlisting}

Write a function called \li{bin_parallel} that uses a parallel implementation to find the furthest address from a recycle bin.
Return the furthest address, its closest bin, and the runtime of your function as a tuple.

The necessary data points have been given as \li{recycle_bins.npy} and \li{ny_addr.npy} with latitude and longitude as rows and records as columns.

Consider the problem of numerical integration using the trapezoidal rule, depicted in Figure \ref{fig:traprule}.
Recall the following formula for estimating an integral using the trapezoidal rule,
\[
\int_{a}^b f(x) dx \approx \frac{h}{2} \sum_{k=1}^N (f(x_{k+1}) + f(x_k)),
\]
where $x_k$ is the $k$th point, and $h$ is the distance between any two points (note they are evenly spaced).

Note that estimation of the area of each interval is independent of all other intervals. 
As a result, this problem is considered to be embarrassingly parallel.

Write a function called \li{parallel_trapezoidal_rule()} that accepts a function handle to integrate, bounds of integration, and the number of points to use for the approximation. 
Utilize what you have learned about parallel computing to parallelize the trapezoidal rule in order to estimate the integral of $f$. 
That is, evenly divide the points among all available processors and run the trapezoidal rule on each portion simultaneously.
The sum of the results of all the processors will be the estimation of the integral over the entire interval of integration.
Return this sum. 

\begin{figure}[H]

\begin{center}
		
\begin{tikzpicture}[scale=1.5]
%Draw the axes
\draw[->,>=stealth',thick] (-.5,0) -- (5,0);
\draw[->,>=stealth',thick] (0,-.5) -- (0,5);
%Draw the ticks and labels
\foreach \x/\t in {.5/x_1,1.5/x_2,2.5/x_3,3.5/x_4,4.5/x_5} \draw (\x,-3pt) -- (\x,0) node[anchor=north, yshift=-.25cm] {$\t$}; 
%Labels
%\node[align=center] at (1.3,4) {Trapezoid Rule\\Uniform Partitioning};
\node[anchor=east,xshift=-.2cm] at (0,4.5) {$y$};
\node[anchor=north,yshift=-.25cm] at (5,-.2) {$x$};
\node[yshift=-.25cm] at (3,3.5) {$y=f(x)$};
%Draw the curve
\draw[thick] (.5,2) sin (1.5,2.7) cos (2.5,2.5) sin (3.5,2.3) cos (4.5,3);
%Draw the faded vertical lines
\foreach \x/\y in {.5/2,1.5/2.7,2.5/2.5,3.5/2.3,4.5/3} \draw[help lines] (\x,0) -- (\x,\y);
%The h measure and label
\draw[thick] (1.5,1.4) -- (1.5,1.6) |- (2,1.5) node[below] {$h$} -| (2.5,1.4) -- (2.5,1.6) ;
%Draw the red lines
\foreach \x/\y/\s/\t in {.5/2/1.5/2.7,1.5/2.7/2.5/2.5,2.5/2.5/3.5/2.3,3.5/2.3/4.5/3} \draw[red,thick] (\x,\y) -- (\s,\t);

\end{tikzpicture}
\end{center}
\label{fig:traprule}
\caption{A depiction of the trapezoidal rule with uniform partitioning.}
\end{figure}



%All of the examples that we have done in this lab up to this point may have seemed quite simplistic. However, we will now apply all these examples to a real-world example.
%
%Many natural language processing (NLP) problems naturally extend to a parallel computing architecture. In many situations, we will have a function we want to apply to a piece of text, whether that be a sentence, paragraph, or an entire document. An efficient way to handle these problems is to scatter the data to all available engines, perform the calculations, then gather the results.
%
%The introductory step to latent symantic analysis is to create a occurance matrix based on the bag-of-words model. At a high level, the bag-of-words model gives us insight as to the topic of a given document. It can also be used to measure the similarity between two documents.
%
%The occurrence matrix has the ids for the different documents as the rows and the different words in the vocabulary as the columns. Then the $ij$-th entries of this matrix is the number of times the $j$-th word appears in the $i$-th document. For example, consider the following example:
%
%Say we have the following 4 pieces of text:
%\begin{enumerate}
%    \item the rose is red
%    \item the violet is blue
%    \item my car is red
%    \item i have a red rose and a red car
%\end{enumerate}
%
%It is common to remove \emph{stopwords} or the most common words in the language. We can get a list of stopwords by running:
%\begin{lstlisting}
%>>> from nltk.corpus import stopwords
%>>> set_stopwords = set(stopwords.words('english'))
%\end{lstlisting}
%
%After removing the stopwords, the resulting occurrence matrix would be:
%\begin{lstlisting}
%      'blue' 'car' 'red' 'rose' 'violet'
%doc1     0     0     1      1       0
%doc2     1     0     0      0       1
%doc3     0     1     1      0       0
%doc4     0     1     2      1       0
%\end{lstlisting}
%
%Notice that the columns contain all the non-stopwords that were used throughout all the documents combined.
%
%The \li{state_union.zip} file contains all of the State of the Union addresses from 1945-2006. For this problem, leverage the power of parallel computing to create an occurrence matrix where the rows are the different State of the Union addresses and the columns are the words in the collective vocabulary. There are many ways to tackle this problem, so we will leave it open-ended. However, we strongly recommend that you base your solution around the \li{scatter()} and \li{gather()} methods.
%
%For the sake of grading, order the rows chronologically and the columns in alphabetical order. Also, as you can imagine, this matrix will end up being fairly sparse, so it is more efficient to use \li{scipy.sparse} matrices.
%
If you are working on a Linux or Mac computer, open a terminal and execute the \li{htop} command. (If \li{htop} is not on your system, install it using your default package manager). 
When opening this program, your terminal should see an interface similar to Figure \ref{fig:htop}. 
The numbered bars at the top represent each of the cores of your processor and the workload on each of these cores.

Now, run the following Python code with your terminal running \li{htop} still visible. 
The sole purpose of the following code is to create a computationally intensive function that runs for about 15 seconds.

\begin{lstlisting}
import numpy as np
for i in range(10000):
    np.random.random(100000)
\end{lstlisting}

You should have seen one or two (if your operating system has some built in load balancing) of the cores run substantially higher than the others. 
%It is also possible that you saw the load-carrying core switch midway through the execution of the file. 
This is an indicator that our script is being executed in serial -- one line at a time, one core at a time.

Initialize an IPython cluster with an engine for each of your machines processor cores. 
As you did in the previous problem, open \li{htop}. 
Run the following code and examine what happens in htop.

\begin{lstlisting}
from ipyparallel import Client
client = Client()
dview = client[:]

dview.execute("""
import numpy as np
for i in range(10000):
    np.random.random(100000)
""")
\end{lstlisting}

The output of \li{htop} should appear similar to Figure \ref{fig:htop_cluster}. 
Notice that all of the processors are being utilized to run the script.

Now let's do a problem that is a bit more computationally intensive. 
Define the random variable $X$ to be the maximum out of $N$ draws from the standard normal distribution.
For example, one draw from $X$ when $N$ = 10 would be the maximum out of 10 draws from the normal distribution.
Write a function that accepts an integer $N$, takes 500,000 draws from this distribution ($X$), and plots the draws in a histogram.
The resulting histogram will approximate the p.d.f. of $X$.

Write your function in such a way that each engine will carry an equal load. Also write your function in such a way that it is flexible to the number of engines that are running. HINT: Remember that you can get a list of all available engines using \li{clients.ids}.
